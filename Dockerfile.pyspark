FROM bitnami/spark:3.5.0

# Directorio de trabajo
WORKDIR /app

# Copiar el script de la carpeta dags/scripts al contenedor
COPY dags/scripts/etl_script.py ./etl_script.py

# Instalar python-dotenv
RUN pip install --no-cache-dir python-dotenv

# Comando para ejecutar Spark con las dependencias de AWS S3
CMD ["spark-submit", "--master", "local[*]", "--packages", "org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.901", "etl_script.py"]
